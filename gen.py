import sys
import requests
import json
import time
import tqdm

def generate_response_with_ollama(prompt):
    try:
        # Display progress bar while processing
        with tqdm.tqdm(total=100, desc="Generating response", ncols=100) as pbar:
            for _ in range(10):
                time.sleep(0.1)  # Simulate progress
                pbar.update(10)
        
        # Call the Ollama API with the appropriate endpoint and model
        response = requests.post(
            'http://localhost:11434/api/generate',  # Update to correct Ollama endpoint
            headers={'Content-Type': 'application/json'},
            data=json.dumps({
                "model": "llama3",  # Updated model version
                "prompt": prompt,
                "format": "json",
                "stream": False
            })
        )
        response.raise_for_status()
        response_data = response.json()
        if 'outputs' in response_data and len(response_data['outputs']) > 0:
            return response_data['outputs'][0].get('text', '').strip()
        else:
            return "No valid response generated by Ollama."
    except requests.exceptions.RequestException as e:
        print(f"Error calling Ollama API: {e}")
        return "Failed to generate response with Ollama."

if __name__ == "__main__":
    prompt = sys.argv[1]
    response = generate_response_with_ollama(prompt)
    print(response)